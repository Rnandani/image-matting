\textbf{Gradient descent} is another iterative method for solving systems of linear equations. The general idea is that we take each alpha value in our matte to be a component of a large vector. We then have some $n$-dimensional space, $\mathcal{D}$, that contains all possible configurations of our alpha matte, one (or more) of which we want to find. We start with some guess vector, $\textbf{u}$, and at each iteration we find a correction vector $\textbf{d}$ such that $\textbf{u}^{k+1} = \textbf{u}^k+\textbf{d}$ (where $k$ indicates the iteration) takes us closer to our desired solution. The problem is then finding our $\textbf{d}$. To do this, we note that the gradient of the system of equations we are solving slopes towards local minima. Then by taking steps in the direction of the gradient, we can approach our minimum with each step.
\\\\
We now recall that our original problem, prior to creating a system of linear equations, is in fact the quadratic form $J(\alpha)=\alpha^TL\alpha$. Then solving our system of linear equations is identical to minimizing this quadratic form. Further, the gradient of $J(\alpha)$ is just $L\alpha$, our residual. Since any linear optimization problem like this can be rewritten similarly as minimizing a quadratic form, we can take our correction vector in general to be $\textbf{d}=\textbf{f}-A\textbf{u}$.
\\\\
\textbf{Conjugate gradient descent} is a slight modification of this algorithm. In traditional gradient descent, many steps might be taken in the same direction, and as such a large number of steps might be redundant. CG fixes this by requiring that each step must be \textbf{conjugate} (\textbf{A-orthogonal}) to the previous step. This means that
\[\left\langle \textbf{u}^{k+1},\textbf{u}^k\right\rangle_A = \left(\textbf{u}^{k+1}\right)^TA\textbf{u}^k=0\]
Then subsequent steps don't make corrections in the same direction, so many redundant steps are eliminated. In practice, this is very consistenly the case. CG converges at a much faster rate then normal gradient descent. To produce the CG values of $d$, a simple application of Gram-Schmidt orthoganalization suffices.
\\\\
This method is very common in optimization problems, and as such was tried on its own in \cite{lee14}. However, its results were barely comparable to the V-Cycle algorithm, generally taking at least an order of magnitude more iterations before reasonable convergence. A multigrid variant of this technique had more promising results.