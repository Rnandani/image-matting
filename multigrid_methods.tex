Multigrid methods were invented as a solution to poor convergence behavior on low frequency modes. The idea is to downsample the system of linear equations, such that the low frequency modes which occur at high resolution become high frequency modes at coarse resolution, and hence converge quickly. Fortuitously, the mathematical basis built up for our system of equations translates well into this framework. We refer back to the notation used in the interpolation section, $\textbf{v}^h$ referring to $\textbf{v}$ at resolution $h$, as this notation is used extensively in this section.
\\\\
The general algorithm for a multigrid is to produce a high resolution error, then downsample it, correct as much error as possible (through iteration) at this coarse level, then upsample and apply the error corrections to the higher level of detail. Naively, we can summarize this as starting with a good $\textbf{v}^h$ by first solving (or approximating) $\textbf{v}^{16h}$ or some variant, then applying our interpolation operator several times to produce our more detailed error. Unfortunately, this simple algorithm is not easy to apply when we are given an initial guess, $\textbf{v}^{(0)}$, at the highest level of resolution. However, the residual equation here comes to the rescue:
\[A\textbf{e}=\textbf{r}\hspace{.5in} A^{h}\textbf{e}^{h}=\textbf{r}^{h}\]
Our $A^{h}$ and $\textbf{r}^{h}$ are known, and the initial guess for $\textbf{e}^{h}$ is $\textbf{0}$, regardless of the $\textbf{v}^{(0)}$ chosen. Further, this is yet another system of linear equations, involving smaller values, and with a simpler convergent case. This leads us to the first candidate for a multigrid algorithm:
\vspace{-.2in}
\begin{quote}
\item
\subparagraph{Nested Iteration \cite{lee14}}
\begin{enumerate}
\item Calculate $r^h$ from $f^h-A^h{v}^{(n)h}$\\
(note that $\textbf{v}^{(n)h}$ is a combination of notations $\textbf{v}^{(n)}$ and $\textbf{v}^h$).
\item Calculate $r^{2h}$ from the restriction operation $I_{h}^{2h}r^h$.
\item Solve $A^{2h}\textbf{e}^{2h}=r^{2h}$ for $\textbf{e}^{2h}$ by Nested Iteration with $\textbf{e}^{(0)2h}=0$.
\item Calculate $\textbf{v}^{(n+1)h}$ from
	$\textbf{v}^{(n)h}+I_{2h}^he^{2h}$
\end{enumerate}
\end{quote}
This method is quite effective, but only if the error is relatively smooth. Namely, the interpolation operation at the final step must make a good approximation of the actual error. However, we now have an algorithm that preserves corrections to smooth error but not oscillatory error. Clearly, combining this with a relaxation method such as Gauss-Seidel could have good results.
\\\\
This combination leads to what is known as the \textbf{V-Cycle Method}, so called for the V-like shape in which it first restricts repeately, then interpolates repeatedly. Following is the pseudocode for this algorithm in \cite{lee14}:
\vspace{-.2in}
\begin{quote}
\item
\subparagraph{V-Cycle Pseuodocode}
\begin{enumerate}
\item[1:] function VCYCLE$_H$($\textbf{u}^h$,$f^h$):
\item[2:] \hspace{.2in} if $h=H$:
\item[3:] \hspace{.4in} return final solution $\textbf{u}^h$
\item[4:] \hspace{.2in} Relax on $A^h\textbf{u}^h=f^h$
\item[5:] \hspace{.2in} $r^{h}\leftarrow f^h-
						A^h\textbf{u}^h$
\item[6:] \hspace{.2in} $\textbf{e}^{2h}\leftarrow$
						VCYCLE$_H(\textbf{e}^{2h}=0,
						I_h^{2h}r^h)$
\item[7:] \hspace{.2in} $\textbf{u}_h\leftarrow
						\textbf{u}^h+I_{2h}^h
						\textbf{e}^{2h}$
\item[8:] \hspace{.2in} Relax on $A^h\textbf{u}^h=f^h$
\item[9:] \hspace{.2in} return $\textbf{u}^h$
\end{enumerate}
\end{quote}
In fact, this algorithm is guaranteed to converge in a fixed number of iterations (conditioned on $n^2$) \cite{gopal08}.